---
full_title: "GT 6.0 GRAM5 Admin Guide"
short_title: "Admin Guide"
---
:toc:
:toc-placement: manual
:toclevels: 3
:toc-title:
:numbered:
:revdate: September 30, 2016

= GT 6.0 GRAM5 Admin Guide

[red]#(CONTENT NEEDS REVIEW)#

This guide contains configuration information for system administrators working with GRAM5. It describes procedures typically performed by system administrators, including GRAM5 software installation, configuration, testing, and debugging. Readers should be familiar with the GRAM5 Key Concepts to understand the motivation for and interaction between the various deployed components.

'''
toc::[]

== GRAM5 Installation
=== Introduction
The Globus Toolkit provides GRAM5: a service to submit, monitor, and cancel jobs on Grid computing resources. In GRAM5, a job consists of a computation and, optionally, file transfer and management operations related to the computation. Some users, particularly interactive ones, benefit from accessing output data files as the job is running. Monitoring consists of querying for and/or subscribing to status information, such as job state changes.

GRAM5 relies on link:../../gsi[GSI C] mechanisms for security, and interacts with link:../../gridftp[GridFTP] services to stage files to compute resources. Please see their respective Administrator’s guides for information about installing, configuring, and managing those systems. In particular, you must understand the tasks in link:../../installation[Installing GT 6.0] and install the basic GRAM5 packages, and complete the tasks in link:../../installation#basic_security_configuration[Basic Security Configuration].

=== Planning your GRAM5 installation
Before installing GRAM5 on a server, you’ll first need to plan what Local Resource Managers (LRMs) you want GRAM5 to interface with, what LRM you want to have as your default GRAM5 service, and whether you’ll be using the `globus-scheduler-event-generator` to process LRM events.

GRAM5 requires a few services to be running to function: the Gatekeeper and the Scheduler Event Generator (SEG). The supported way to run these services is via the System-V style init scripts provided with the GRAM5-related packages. The gatekeeper daemon can also be configured to start via an internet superserver such as `inetd` or `xinetd` though that is beyond the scope of this document. The `globus-scheduler-event-generator` can not be run in that way.

==== Choosing an LRM Adapter

GRAM5 in GT 6.0 supports the following LRM adapters: Condor, PBS, GridEngine, and Fork. These LRM adapters translate GRAM5 job specifications into LRM-specific job descriptions and scripts to run them, as well as interfaces to the LRM to determine job termination status.

If you’re not familiar with the supported LRMs, you might want to start with the Fork one to get familiar with how GRAM5 works. This adapter simply forks the job and runs it on the GRAM5 node. You can then install one of the other LRMs and its adapter to provide batch or high-throughput job scheduling.

===== Default GRAM5 Service

GRAM5 can be configured to support multiple LRMs on the same service machine. In that case, one LRM is typically configured as the default LRM which is used when a client uses a shortened version of a GRAM5 link:../user#resource_name[resource name]. A common configuration is to configure a batch system interface as the default, and provide the [monospaced]#jobmanager-fork# service as well for simple jobs, such as creating directories or staging data.

===== Job Status Method

GRAM5 has two ways of determining job state transitions: polling the LRM and using the Scheduler Event Generator (SEG) service. When polling, each user’s `globus-job-manager` will periodically execute an LRM-specific command to determine the state of each job. On systems with many users, or with users submitting a large number of jobs, this can cause significant resource use on the GRAM5 service machine. Instead, the GRAM5 service can be configured (on a per-LRM basis) to use the `globus-scheduler-event-generator` service to more efficiently process LRM state changes. [NOTE]

Not all LRM adapters provide an interface to the `globus-scheduler-event-generator`, and some require LRM-specific configuration to work properly. This is described in more detail.

=== Installing LRM Adapter Package
There are several LRM adapters included in the GT 6.0. For some, there is a [file]#-setup-poll# and [file]#-setup-seg# package which installs the adapter and configuration file needed for job status via polling or the `globus-scheduler-event-generator` program.

There are three ways to get LRM adapters: as RPM packages, as Debian packages, and from the source installer. These installation methods are described in Installing GT 6.0.

LRM adapter packages included in the GT 6.0 release are:

.GRAM5 LRM Adapters
[cols="4*<",options="header"]
|========
|LRM Adapter	|Poll Package	|SEG Package	|Installer Target
|fork	|globus-gram-job-manager-fork-setup-poll	|globus-gram-job-manager-fork-setup-seg ^[link:#note1[a]]^	|globus_gram_job_manager_fork
|link:http://www.clusterresources.com/products/torque-resource-manager.php[pbs]	|globus-gram-job-manager-pbs-setup-poll	|globus-gram-job-manager-pbs-setup-seg	|globus_gram_job_manager_pbs
|link:http://www.cs.wisc.edu/condor/[Condor]	|N/A	|globus-gram-job-manager-condor ^[link:#note2[b]]^	|globus_gram_job_manager_condor
|GridEngine	|globus-gram-job-manager-sge-setup-poll	|globus-gram-job-manager-sge-setup-seg	|globus_gram_job_manager_sge
4+|[[note1]]^[a]^ Not recommended for production use
4+|[[note2]]^[b]^ This LRM uses a SEG-like mechanism included in the `globus-job-manager program`, but not the `globus-scheduler-event-generator` service.
|========

== Common Administrative Tasks
There are several tools provided with GT 6.0 to manage GRAM5, as well as OS-specific tools to start and stop some of the services. There are tools to manage user authorization, which services are enabled, which scheduler event generator modules are enabled, and to test the `globus-gatekeeper` service.

=== Managing GRAM5 Users
Before a user may interact with the GRAM5 service to submit jobs, he or she must be authorized to use the service. In order to be authorized, a GRAM5 administrator must add the user’s credential name and local account mapping to the [file]#/etc/grid-mapfile#. This can be done using the `grid-mapfile-add-entry` and `grid-mapfile-delete-entry` tools. For more information, see the link:../../gsi[GSI C] manual.

=== Starting and Stopping GRAM5 services
In order to run the service, the `globus-gatekeeper`, and, if applicable to your configuration, the `globus-scheduler-event-generator` services must be running on your system. The packages for these services include init scripts and configuration files which can be used to configure, start, and stop the service.

The `globus-gatekeeper` and `globus-scheduler-event-generator` init scripts handle the following actions: `start`, `stop`, `status`, `restart`, `condrestart`, `try-restart`, `reload`, and `force-reload`. The `globus-scheduler-event-generator` script also accepts another optional parameter to start or stop a particular `globus-scheduler-event-generator` module. If the second parameter is not present, then all services will be acted on.

==== Debian Specifics

If you installed using Debian packaging tools, then the services will automatically be started upon installation. To start or stop the service, use the command `invoke-rc.d` with the service name and action.

==== RPM Specifics

If you installed using the RPM packaging tools, then the services will be installed but not enabled by default. To enable the services to start at boot time, use the commands:

----terminal
#  chkconfig globus-gatekeeper on
#  chkconfig globus-scheduler-event-generator on
----terminal

To start or stop the services, use the `service` command to run the init scripts with the service name and action and optional `globus-scheduler-event-generator` module.

=== Enabling and Disabling GRAM5 Services
The GRAM5 packages described in link:#installing_lrm_adapter_packages[Installing LRM Adapter Packages] will automatically register themselves with the `globus-gatekeeper` and `globus-scheduler-event-generator` services. The first LRM adapter installed will be configured as the default Job Manager service. To list the installed services, change the default, or disable a service, use the `globus-gatekeeper-admin` tool.

.Example 2.1. Using globus-gatekeeper-admin to set the default service

This example shows how to use the `globus-gatekeeper-admin` tool to list the available services and then choose one as the default:

----terminal
#  globus-gatekeeper-admin -l
[output]#jobmanager-condor [ENABLED]
jobmanager-fork-poll [ENABLED]
jobmanager-fork [ALIAS to jobmanager-fork-poll]#
#  globus-gatekeeper-admin -e jobmanager-condor -n jobmanager
#  globus-gatekeeper-admin -l
[output]#jobmanager-condor [ENABLED]
jobmanager-fork-poll [ENABLED]
jobmanager [ALIAS to jobmanager-condor]
jobmanager-fork [ALIAS to jobmanager-fork-poll]#
----terminal

=== Enabling and Disabling SEG Modules
The `-setup-seg` packages described in link:#installing_lrm_adapter_packages[Installing LRM Adapter Packages] will automatically register themselves with the `globus-scheduler-event-generator` service. To disable a module from running when the `globus-scheduler-event-generator` service is started, use the globus-scheduler-event-generator-admin tool.

.Using globus-scheduler-event-generator-admin to disable a SEG module
This example shows how to stop the pbs globus-scheduler-event-generator module and disable it so it will not restart when the system is rebooted:

----terminal
#  /etc/init.d/globus-scheduler-event-generator stop pbs
[output]#Stopped globus-scheduler-event-generator                   [  OK  ]#
#  globus-scheduler-event-generator-admin -d pbs
#  globus-scheduler-event-generator-admin -l
[output]#pbs [DISABLED]#
----terminal

== Configuring GRAM5
GRAM5 is designed to be usable by default without any manual configuration. However, there are many ways to customize a GRAM5 installation to better interact with site policies, filesystem layouts, LRM interactions, logging, and auditing. In addition to GRAM5-specific configuration, see link:../../gsi/admin#configuring[Configuring GSI] for information about configuring GSI security.

=== Gatekeeper Configuration
The `globus-gatekeeper` has many configuration options related to network configuration, security, logging, service path, and nice level. This configuration is located in:

RPM Package::
[output]#/etc/sysconfig/globus-gatekeeper#
Debian Package::
[output]#/etc/default/globus-gatekeeper#
Source Installer::
[output]#_PREFIX_/etc/globus-gatekeeper.conf#

The following configuration variables are available in the `globus-gatekeeper` configuration file:

GLOBUS_GATEKEEPER_PORT::
Gatekeeper Service Port. If not set, the `globus-gatekeeper` uses the default of [output]#2119#.
GLOBUS_LOCATION::
Globus Installation Path. If not set, the `globus-gatekeeper` uses the paths defined at package compilation time.
GLOBUS_GATEKEEPER_LOG::
Gatekeeper Log Filename. If not set, the `globus-gatekeeper` logs to syslog using the [output]#GRAM-gatekeeper# log identification prefix. The default configuration value is [output]#/var/log/globus-gatekeeper.log#
GLOBUS_GATEKEEPER_GRID_SERVICES::
Path to grid service definitions. If not set, the `globus-gatekeeper` uses the default of [output]#/etc/grid-services.#.
GLOBUS_GATEKEEPER_GRIDMAP::
Path to grid-mapfile for authorization. If not set, the `globus-gatekeeper` uses the default of [output]#/etc/grid-security/grid-mapfile.#.
GLOBUS_GATEKEEPER_CERT_DIR::
Path to a trusted certificate root directory. If not set, the `globus-gatekeeper` uses the default of [output]#/etc/grid-security/certificates.#.
GLOBUS_GATEKEEPER_CERT_FILE::
Path to the gatekeeper’s certificate. If not set, the `globus-gatekeeper` uses the default of [output]#/etc/grid-security/hostcert.pem.#.
GLOBUS_GATEKEEPER_KEY_FILE::
Path to the gatekeeper’s private key. If not set, the `globus-gatekeeper` uses the default of [output]#/etc/grid-security/hostkey.pem.#.
GLOBUS_GATEKEEPER_KERBEROS_ENABLED::
Flag indicating whether or not the `globus-gatekeeper` will use a kerberos GSSAPI implementation instead of the GSI GSSAPI implementation (untested).
GLOBUS_GATEKEEPER_KMAP::
Path to the KMAP authentication module. (untested).
GLOBUS_GATEKEEPER_PIDFILE::
Path to a file where the `globus-gatekeeper` 's process ID is written. If not set, `globus-gatekeeper` uses [output]#/var/run/globus-gatekeeper.pid#
GLOBUS_GATEKEEPER_NICE_LEVEL::
Process nice level for `globus-gatekeeper` and `globus-job-manager` processes. If not set, the default system process nice level is used.

After modifying the configuration file, restart the `globus-gatekeeper` using the methods described in link:../../gram5/admin#starting_and_stopping_gram5_services[Starting and Stopping GRAM5 services].

=== Scheduler Event Generator Configuration
The `globus-scheduler-event-generator` has several configuration options related to filesystem paths. This configuration is located in:

RPM Package::
[output]#/etc/sysconfig/globus-scheduler-event-generator#
Debian Package::
[output]#/etc/default/globus-scheduler-event-generator#
Source Installer::
[output]#PREFIX/etc/globus-scheduler-event-generator.conf#

The following configuration variables are available in the `globus-scheduler-event-generator` configuration file:

GLOBUS_SEG_PIDFMT::
Scheduler Event Generator PID file path format. Modify this to be the location where the `globus-scheduler-event-generator` writes its process IDs (one per configured LRM). The format is a `printf` format string with one %s to be replaced by the LRM name. By default, `globus-scheduler-event-generator` uses [output]#/var/run/globus-scheduler-event-generator-%s.pid.#.
GLOBUS_SEG_LOGFMT::
Scheduler Event Generator Log path format. Modify this to be the location where `globus-scheduler-event-generator` writes its event logs. The format is a `printf` format string with one %s to be replaced by the LRM name. By default, `globus-scheduler-event-generator` uses [output]#/var/lib/globus/globus-seg-%s#. If you modify this value, you’ll need to also update the LRM configuration file to look for the log file in the new location. If you modify this value, you’ll need to also update the LRM configuration file to look for the log file in the new location.
GLOBUS_SEG_NICE_LEVEL::
Process nice level for `globus-scheduler-event-generator processes`. If not set, the default system process nice level is used.

After modifying the configuration file, restart the `globus-scheduler-event-generator` using the methods described in link:../../gram5/admin#starting_and_stopping_gram5_services[Starting and Stopping GRAM5 services].

=== Job Manager Configuration
The `globus-job-manager` process is started by the `globus-gatekeeper` and uses the configuration defined in the service entry for the resource name. By default, these service entries use a common configuration file for most job manager features. This configuration is located in:

RPM Package::
[output]#/etc/globus/globus-gram-job-manager.conf#
Debian Package::
[output]#/etc/globus/globus-gram-job-manager.conf#
Source Installer::
[output]#PREFIX/etc/globus-gram-job-manager.conf#

This configuration file is used to construct the command-line options for the `globus-job-manager` program. Thus, all of the options described in link:#globus_job_manager_8[`globus-job-manager`] may be used.

==== Job Manager Logging
From an administrator’s perspective, the most important job manager configuration options are likely the ones related to logging and auditing. The default GRAM5 configuration puts logs in [output]#/var/log/globus/gram_USERNAME.log#, with logging enabled at the `FATAL` and `ERROR` levels. To enable more fine-grained logging, add the option `-log-levels` ' to [output]#/etc/globus/globus-gram-job-manager#.conf. The value for . The value for 'LEVELS is a set of log levels joined by the | character. The available log levels are:

.GRAM5 Log Levels
[cols="3*<",options="header"]
|========
|Level	|Meaning	|Default Behavior
|`FATAL`	|Problems which cause the job manager to terminate prematurely.	|Enabled
|`ERROR`	|Problems which cause a job or operation to fail.	|Enabled
|`WARN`	|Problems which cause minor problems with job execution or monitoring.	|Disabled
|`INFO`	|Major events in the lifetime of the job manager and its jobs.	|Disabled
|`DEBUG`	|Minor events in the lifetime of jobs.	|Disabled
|`TRACE`	|Job processing details.	|Disabled
|========

In RPM or Debian package installs, these logs will be configured to be rotated via `logrotate`. See [output]#/etc/logrotate.d/globus-job-manager# for details on the default log rotation configuration.

==== Firewall Configuration

There are also a few configuration options related to the TCP ports the the Job Manager users. This port configuration is useful when dealing with firewalls that restrict incoming or outgoing ports. To restrict incoming ports (those that the Job Manager listens on), add the command-line option `-globus-tcp-port-range` to the Job Manager configuration file like this:

----
-globus-tcp-port-range MIN-PORT,MAX-PORT
----

Where _MIN-PORT_ is the minimum TCP port number the Job Manager will listen on and _MAX-PORT_ is the maximum TCP port number the Job Manager will listen on.

Similarly, to restrict the outgoing port numbers that the job manager connects form, use the command-line option `-globus-tcp-source-range`, like this:

----
-globus-tcp-source-range MIN-PORT,MAX-PORT
----

Where _MIN-PORT_ is the minimum outgoing TCP port number the Job Manager will use and _MAX-PORT_ is the maximum TCP outgoing port number the Job Manager will use.

For more information about Globus and firewalls, see link:../../installation#firewall_configuration[Firewall configuration].

=== LRM Adapter Configuration
Each LRM adapter has its own configuration file which can help customize the adapter to the site configuration. Some LRMs use non-standard programs to launch parallel or MPI jobs, and some might want to provide queue or project validation to make it easier to translate job failures into problems that can be described by GRAM5. All of the LRM adapter configuration files consist of simple `variable="value"` pairs, with a leading # starting a comment until end-of-line.

Generally, the GRAM5 LRM configuration files are located in the globus configuration directory, with each configuration file named by the LRM name (`fork`, `condor`, `pbs`, `sge`, or `slurm`). The following are the paths to these configurations:

RPM Package::
/etc/globus/globus-_LRM_.conf
Debian Package::
/etc/globus/globus-_LRM_.conf:
Source Installer::
_PREFIX_/etc/globus/globus-_LRM_.conf

==== Fork

The [file]#globus-fork.conf# configuration file can define the following configuration parameters:

log_path::
Path to the [file]#globus-fork.log# file used by the file used by the `globus-fork-starter` and fork SEG module.
mpiexec, mpirun::
Path to `mpiexec` and `mpirun` for parallel jobs which use MPI. By default, these are not configured. The LRM adapter will use `mpiexec` over `mpirun` if both are defined.
softenv_dir::
Path to an installation of link:http://www.mcs.anl.gov/hs/software/systems/softenv/softenv-intro.html[softenv], which is used on some systems to manage application environment variables.

==== Condor

The [file]#globus-condor.conf# configuration file can define the following configuration parameters:

condor_os::
Custom value for the OpSys requirement for condor jobs. If not specified, the system-wide default will be used.
condor_arch::
Custom value for the OpSys requirement for condor jobs. If not specified, the system-wide default will be used.
condor_submit, condor_rm::
Path to the condor commands that the LRM adapter uses. These are usually determined when the LRM adapter is compiled if the commands are in the [output]#PATH#.
condor_config::
Value of the `CONDOR_CONFIG` environment variable, which might be needed to use condor in some cases.
check_vanilla_files::
Enable checking if executable, standard input, and directory are valid paths for `vanilla` universe jobs. This can detect some types of errors before submitting jobs to condor, but only if the filesystems between the condor submit host and condor execution hosts are equivalent. In other cases, this may cause unneccessary job failures.
condor_mpi_script::
Path to a script to launch MPI jobs on condor

==== PBS

The globus-pbs.conf configuration file can define the following configuration parameters: configuration file can define the following configuration parameters:

log_path::
Path to PBS server_logs directory. The PBS SEG module parses these logs to generate LRM events.
pbs_default::
Name of the PBS server node, if not the same as the GRAM service node.
mpiexec, mpirun::
Path to mpiexec and mpirun for parallel jobs which use MPI. By default these are not configured. The LRM adapter will use `mpiexec` over `mpirun` if both are defined.
qsub, qstat, qdel::
Path to the LRM-specific command to submit, check, and delete PBS jobs. These are usually determined when the LRM adapter is compiled if they are in the PATH.
cluster::
If this value is set to yes, then the LRM adapter will attempt to use a remote shell command to launch multiple instances of the executable on different nodes, as defined by the file named by the `PBS_NODEFILE` environment variable.
remote_shell::
Remote shell command to launch processes on different nodes when cluster is set to yes.
cpu_per_node::
Number of instances of the executable to launch per allocated node.
softenv_dir::
Path to an installation of link:http://www.mcs.anl.gov/hs/software/systems/softenv/softenv-intro.html[softenv] which is used on some systems to manage application environment variables.

==== SGE

The globus-sge.conf configuration file can define the following configuration parameters: configuration file can define the following configuration parameters:

sge_root
Root location of the GridEngine installation. If this is set to [output]#undefined#, then the LRM adapter will try to determine it from the `globus-job-manager` environment, or if not there, the contents of the file named by the `sge_config` configuration parameter.
sge_cell
Name of the GridEngine cell to interact with. If this is set to [output]#undefined#, then the LRM adapter will try to determine it from the `globus-job-manager` environment, or if not there, the contents of the file named by the `sge_config` configuration parameter.
sge_config
Path to a file which defines the `SGE_ROOT` and the `SGE_CELL` environment variables.
log_path
Path to GridEngine reporting file. This value is used by the SGE SEG module. If this is used, GridEngine must be configured to write a reporting file and not load reporting data into an ARCo database.
qsub, qstat, qdel, qconf
Path to the LRM-specific command to submit, check, and delete GridEngine jobs. These are usually determined when the LRM adapter is compiled if they are in the PATH.
sun_mprun, mpirun
Path to mprun and mpirun for parallel jobs which use MPI. By default these are not configured. The LRM adapter will use mprun over mpirun if both are defined.
default_pe
Default parallel environment to submit parallel jobs to. If this is not set, then clients must use the parallel_environment RSL attribute to choose one.
validate_pes
If this value is set to yes, then the LRM adapter will verify that the `parallel_environment` RSL attribute value matches one of the parallel environments supported by this GridEngine service.
available_pes
If this value is defined, use it as a list of parallel environments supported by this GridEngine deployment for validation when validate_pes is set to yes. If validation is being done but this value is not set, then the LRM adapter will query the GridEngine service to determine available parallel environments at startup.
default_queue
Default queue to use if the job description does not name one.
validate_queues
If this value is set to yes, then the LRM adapter will verify that the queue RSL attribute value matches one of the queues supported by this GridEngine service.
available_queues
If this value is defined, use it as a list of queues supported by this GridEngine deployment for validation when validate_queues is set to yes. If validation is being done but this value is not set, then the LRM adapter will query the GridEngine service to determine available queues at startup.